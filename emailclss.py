# -*- coding: utf-8 -*-
"""emailclss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EDtR9BAG68wRC7Um_3MMNeVheOijb7sU
"""

!pip install streamlit pyngrok transformers sentence-transformers matplotlib --quiet

# Commented out IPython magic to ensure Python compatibility.
# 
# 
# # =============================
# # 2) Save the Streamlit App
# # =============================
# %%writefile app.py
# import os, re
# from datetime import datetime, timedelta
# from dataclasses import dataclass
# from typing import List, Dict, Any, Tuple
# 
# import pandas as pd
# import numpy as np
# import streamlit as st
# import matplotlib.pyplot as plt
# 
# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
# from sentence_transformers import SentenceTransformer
# from sklearn.metrics.pairwise import cosine_similarity
# 
# # -----------------------------
# # Config
# # -----------------------------
# SUPPORT_KEYWORDS = ["support","query","request","help","issue","unable","problem",
#     "ticket","error","bug","urgent","asap","immediately","cannot access"]
# 
# URGENCY_KEYWORDS = ["urgent","immediately","asap","cannot access","down","failed",
#     "critical","blocked","refund immediately","escalate"]
# 
# SENTIMENT_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"
# GEN_MODEL = "google/flan-t5-small"
# EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
# CSV_PATH = "/content/68b1acd44f393_Sample_Support_Emails_Dataset.csv"  # <-- your file
# 
# # -----------------------------
# # Data structures
# # -----------------------------
# @dataclass
# class EmailItem:
#     id: str
#     sender: str
#     subject: str
#     body: str
#     received_at: datetime
# 
# @dataclass
# class ProcessedEmail:
#     item: EmailItem
#     sentiment: str
#     priority: str
#     extracted: Dict[str, Any]
#     draft_reply: str
# 
# # -----------------------------
# # Utils
# # -----------------------------
# def contains_any(text: str, keywords: List[str]) -> bool:
#     tl = text.lower()
#     return any(kw in tl for kw in keywords)
# 
# def extract_info(text: str) -> Dict[str, Any]:
#     EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
#     PHONE_RE = re.compile(r"(\+?\d[\d\-\s]{7,}\d)")
#     return {
#         "emails": EMAIL_RE.findall(text),
#         "phones": PHONE_RE.findall(text)
#     }
# 
# @st.cache_resource
# def get_sentiment_pipe():
#     return pipeline("sentiment-analysis", model=SENTIMENT_MODEL)
# 
# @st.cache_resource
# def get_gen_model():
#     tok = AutoTokenizer.from_pretrained(GEN_MODEL)
#     mdl = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)
#     return tok, mdl
# 
# def classify_sentiment(text: str) -> str:
#     senti = get_sentiment_pipe()(text[:512])[0]["label"].lower()
#     mapping = {"positive": "Positive", "neutral": "Neutral", "negative": "Negative"}
#     return mapping.get(senti, senti.title())
# 
# def detect_priority(subject: str, body: str) -> str:
#     urgent = contains_any(subject, URGENCY_KEYWORDS) or contains_any(body, URGENCY_KEYWORDS)
#     return "Urgent" if urgent else "Not Urgent"
# 
# def generate_reply(prompt: str) -> str:
#     tok, mdl = get_gen_model()
#     inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=512)
#     outputs = mdl.generate(**inputs, max_new_tokens=180)
#     return tok.decode(outputs[0], skip_special_tokens=True)
# 
# # -----------------------------
# # Load CSV
# # -----------------------------
# df = pd.read_csv(CSV_PATH)
# emails: List[EmailItem] = []
# for i, row in df.iterrows():
#     emails.append(EmailItem(
#         id=str(i),
#         sender=str(row["sender"]),
#         subject=str(row["subject"]),
#         body=str(row["body"]),
#         received_at=pd.to_datetime(row["sent_date"])
#     ))
# 
# # -----------------------------
# # Process Emails
# # -----------------------------
# processed: List[ProcessedEmail] = []
# for e in emails:
#     sentiment = classify_sentiment(e.body)
#     priority = detect_priority(e.subject, e.body)
#     extracted = extract_info(e.body)
#     prompt = f"Write a professional reply to this email:\n\nSubject: {e.subject}\n\n{e.body}\n\nTone: friendly, empathetic."
#     draft = generate_reply(prompt)
#     processed.append(ProcessedEmail(e, sentiment, priority, extracted, draft))
# 
# processed.sort(key=lambda x: (0 if x.priority=="Urgent" else 1, -x.item.received_at.timestamp()))
# 
# # -----------------------------
# # Streamlit UI
# # -----------------------------
# st.set_page_config(page_title="AI Communication Assistant", layout="wide")
# st.title("üì® AI-Powered Communication Assistant")
# st.caption("Streamlit ‚Ä¢ HuggingFace ‚Ä¢ CSV Inbox ‚Ä¢ Free")
# 
# col1, col2 = st.columns(2)
# col1.metric("Total Emails", len(processed))
# col2.metric("Urgent", sum(1 for p in processed if p.priority=="Urgent"))
# 
# st.subheader("üì• Emails")
# df_show = pd.DataFrame([{
#     "ID": p.item.id,
#     "From": p.item.sender,
#     "Subject": p.item.subject,
#     "Sentiment": p.sentiment,
#     "Priority": p.priority,
#     "Date": p.item.received_at
# } for p in processed])
# st.dataframe(df_show, use_container_width=True)
# 
# st.subheader("‚úçÔ∏è Review & Respond")
# selected = st.selectbox("Choose Email ID", [p.item.id for p in processed])
# sel = next(p for p in processed if p.item.id==selected)
# 
# st.write(f"**From:** {sel.item.sender}")
# st.write(f"**Subject:** {sel.item.subject}")
# st.write(f"**Body:** {sel.item.body}")
# st.write(f"**Sentiment:** {sel.sentiment}")
# st.write(f"**Priority:** {sel.priority}")
# st.write("**Extracted Info:**", sel.extracted)
# 
# reply = st.text_area("Draft Reply", sel.draft_reply, height=200)
# if st.button("‚úÖ Mark as Sent (simulate)"):
#     st.success("Email marked as sent (simulation only).")
# 
# # =============================
# # 3) Run with ngrok
# # =============================
# 
#

from pyngrok import ngrok
ngrok.kill()
!streamlit run app.py --server.port 8501 &>/dev/null&
public_url = ngrok.connect(8501)
print("üåç Public URL:", public_url)



!ngrok config add-authtoken 32Hl2P3TFU97CcvHUdAp9nG5sVR_6K1Te9PXErTd3a8Xvf4nM

!pip install streamlit pyngrok transformers sentence-transformers matplotlib --quiet
!npm install -g localtunnel

!pip install streamlit pyngrok transformers sentence-transformers matplotlib --quiet

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import os, re
# from datetime import datetime
# from dataclasses import dataclass
# from typing import List, Dict, Any
# 
# import pandas as pd
# import streamlit as st
# from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
# 
# # -----------------------------
# # Config
# # -----------------------------
# SUPPORT_KEYWORDS = ["support","query","request","help","issue","unable","problem",
#     "ticket","error","bug","urgent","asap","immediately","cannot access"]
# URGENCY_KEYWORDS = ["urgent","immediately","asap","cannot access","down","failed",
#     "critical","blocked","refund immediately","escalate"]
# 
# SENTIMENT_MODEL = "cardiffnlp/twitter-roberta-base-sentiment-latest"
# GEN_MODEL = "google/flan-t5-small"
# 
# @dataclass
# class EmailItem:
#     id: str
#     sender: str
#     subject: str
#     body: str
#     received_at: datetime
# 
# # -----------------------------
# # Utils
# # -----------------------------
# @st.cache_resource
# def get_sentiment_pipe():
#     return pipeline("sentiment-analysis", model=SENTIMENT_MODEL)
# 
# @st.cache_resource
# def get_gen_model():
#     tok = AutoTokenizer.from_pretrained(GEN_MODEL)
#     mdl = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)
#     return tok, mdl
# 
# def classify_sentiment(text: str) -> str:
#     senti = get_sentiment_pipe()(text[:512])[0]["label"].lower()
#     mapping = {"positive": "Positive", "neutral": "Neutral", "negative": "Negative"}
#     return mapping.get(senti, senti.title())
# 
# def detect_priority(subject: str, body: str) -> str:
#     urgent = any(k in subject.lower()+body.lower() for k in URGENCY_KEYWORDS)
#     return "Urgent" if urgent else "Not Urgent"
# 
# def generate_reply(prompt: str) -> str:
#     tok, mdl = get_gen_model()
#     inputs = tok(prompt, return_tensors="pt", truncation=True, max_length=512)
#     outputs = mdl.generate(**inputs, max_new_tokens=180)
#     return tok.decode(outputs[0], skip_special_tokens=True)
# 
# # -----------------------------
# # Streamlit UI
# # -----------------------------
# st.set_page_config(page_title="AI Communication Assistant", layout="wide")
# st.title("üì® AI-Powered Communication Assistant")
# 
# uploaded_file = st.file_uploader("Upload CSV of emails", type=["csv"])
# if uploaded_file:
#     df = pd.read_csv(uploaded_file)
# 
#     st.success(f"Loaded {len(df)} emails")
# 
#     # Process on demand
#     processed = []
#     for i, row in df.iterrows():
#         e = EmailItem(str(i), row["sender"], row["subject"], row["body"], pd.to_datetime(row["sent_date"]))
#         sentiment = classify_sentiment(e.body)
#         priority = detect_priority(e.subject, e.body)
#         draft = generate_reply(f"Write a professional reply to this email:\n\nSubject: {e.subject}\n\n{e.body}")
#         processed.append({"ID": e.id, "From": e.sender, "Subject": e.subject,
#                           "Sentiment": sentiment, "Priority": priority, "Draft Reply": draft})
# 
#     st.dataframe(pd.DataFrame(processed))
#

!ngrok config add-authtoken 32Hl2P3TFU97CcvHUdAp9nG5sVR_6K1Te9PXErTd3a8Xvf4nM

from pyngrok import ngrok
ngrok.kill()
!streamlit run app.py --server.port 8501 &>/dev/null&
public_url = ngrok.connect(8501)
print("üåç Public URL:", public_url)